<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2013 Unicon (R) Licensed under the
    Educational Community License, Version 2.0 (the "License"); you may
    not use this file except in compliance with the License. You may
    obtain a copy of the License at

    http://www.osedu.org/licenses/ECL-2.0

    Unless required by applicable law or agreed to in writing,
    software distributed under the License is distributed on an "AS IS"
    BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
    or implied. See the License for the specific language governing
    permissions and limitations under the License.

-->
<!-- This is a sample file to explain the format for a pipeline processor configuration file -->
<!-- This file defines an analytics model (or more technically a series of models) including the inputs and outputs -->
<pipeline>
  <type>sample</type><!-- the unique id of the pipeline, should be lowercase alphanum -->
  <name>Sample</name><!-- display/print friendly name -->
  <description>This is a sample pipeline</description>
  <stats>
    <accuracy>0.85</accuracy>
    <confidence>0.05</confidence>
  </stats>

  <sources>
      <source>
          <type>SAMPLECSV</type>
      </source>
  </sources>
  
  <inputs>
    <!--
    There is currently only 1 type of input allowed (temporary storage)
    All input types and fields are defined in the resources/extracts/README.md file
    There are 5 inputs types: PERSONAL, COURSE, ENROLLMENT, GRADE, ACTIVITY
    A field is specified using a combination of the type and the name, for example: COURSE.COURSE_ID or PERSONAL.AGE
    -->
    <fields>
      <!--
       Required fields must be populated (they cannot be blank or null) as accurately as possible
       Optional fields can be blank or null without causing severe inaccuracies in the results
      -->
      <field>
        <name>PERSONAL.AGE</name>
        <required>true</required>
      </field>
      <field>
        <name>COURSE.COURSE_ID</name>
        <required>true</required>
      </field>
      <field>
        <name>COURSE.SUBJECT</name>
        <required>false</required>
      </field>
      <field>
        <name>ENROLLMENT.ALTERNATIVE_ID</name>
        <required>false</required>
      </field>
      <!-- field>
        <name>ENROLLMENT.COURSE_ID</name>
        <required>false</required>
      </field -->
      <field>
        <name>GRADE.GRADABLE_OBJECT</name>
        <required>false</required>
      </field>
      <field>
        <name>ACTIVITY.ID</name>
        <required>false</required>
      </field>
    </fields>
  </inputs>

  <processors>
    <!--
     Processors actually do the work to prepare data or otherwise setup and build and execute the model
     They will execute in the order they are included
    -->
    <processor>
      <type>kettle_job</type>
      <name>kettle_sample_2</name>
      <file>kettle/scoring_sample/Sample_Pipeline1_ETL_MainJob.kjb</file>
    </processor>
    <!-- processor>
      <type>kettle_transform</type>
      <name>kettle_sample_1</name>
      <file>kettle/scoring_sample/LAP_Scoring_Sample.ktr</file>
    </processor -->
    <processor>
      <type>kettle_data</type>
      <name>kettle-data</name>
      <count>100</count>
    </processor>
    <!-- processor>
      <type>fake_data</type>
      <name>AZ-fake-data-sample</name>
      <count>100</count>
    </processor -->
  </processors>

  <outputs>
    <!--
     Outputs do not have to be predefined.
     The pipeline processor will basically extract a field from the temporary storage by name and write it to the indicated location
     Example shown: RISK.SCORE -> a table in persistent storage called RESULTS with a field called RISK_PERCENTAGE
     NOTE: the storage must already have the table/collection defined, this will only attempt to write into it
    -->
<!--     <output> -->
<!--       <type>STORAGE</type>from Temp to Persistent Storage -->
<!--       <from>KETTLE_DATA</from>the temporary storage collection/table -->
<!--       <to>RESULTS</to>the persistent storage collection/table -->
<!--       <fields> -->
<!--         <field> -->
<!--           <source>ALTERNATIVE_ID</source> -->
<!--           <target>ALTERNATIVE_ID</target> -->
<!--         </field> -->
<!--         <field> -->
<!--           <source>ACADEMIC_RISK</source> -->
<!--           <target>ACADEMIC_RISK</target> -->
<!--         </field> -->
<!--       </fields> -->
<!--     </output> -->
    <output>
      <type>CSV</type>
      <from>KETTLE_DATA</from><!-- the temporary storage collection/table -->
      <filename>kettle_data_output.csv</filename>
      <fields>
        <field>
          <header>ALTERNATIVE_ID</header>
          <source>ALTERNATIVE_ID</source>
        </field>
        <field>
          <header>COURSE_ID</header>
          <source>COURSE_ID</source>
        </field>
        <field>
          <header>MODEL_RISK_CONFIDENCE</header>
          <source>MODEL_RISK_CONFIDENCE</source>
        </field>
      </fields>
    </output>
    <output>
      <type>STORAGE</type>
      <from>KETTLE_DATA</from><!-- the temporary storage collection/table -->
      <filename>kettle_data_output.csv</filename>
      <fields>
        <field>
          <header>ALTERNATIVE_ID</header>
          <source>ALTERNATIVE_ID</source>
        </field>
        <field>
          <header>COURSE_ID</header>
          <source>COURSE_ID</source>
        </field>
        <field>
          <header>MODEL_RISK_CONFIDENCE</header>
          <source>MODEL_RISK_CONFIDENCE</source>
        </field>
      </fields>
    </output>
  </outputs>
</pipeline>